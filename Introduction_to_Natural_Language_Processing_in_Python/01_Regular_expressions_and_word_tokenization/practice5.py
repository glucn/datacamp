# Non-ascii tokenization
# In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using
# German with emoji!
#
# Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji
# and the German characters!
#
# The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.
#
# Unicode ranges for emoji are:
#
# ('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), ('\U0001F680-\U0001F6FF'), and
# ('\u2600'-\u26FF-\u2700-\u27BF').

from nltk.tokenize import regexp_tokenize, word_tokenize

from preload import german_text

# Tokenize and print all words in german_text
all_words = word_tokenize(german_text)
print(all_words)

# Tokenize and print only capital words
capital_words = r"[A-Z|Ãœ]\w+"
print(regexp_tokenize(german_text, capital_words))

# Tokenize and print only emoji
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))
